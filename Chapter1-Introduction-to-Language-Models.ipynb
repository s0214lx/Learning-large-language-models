{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bfa263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b1f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba350cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/openbayes/home/huggingface\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f159e",
   "metadata": {},
   "source": [
    "## Hugging Face æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "**Hugging Face** æ˜¯ä¸€ä¸ªè‡´åŠ›äº **äººå·¥æ™ºèƒ½å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰** çš„å¼€æºç¤¾åŒºä¸å¹³å°ï¼Œå…¶æ ¸å¿ƒæ˜¯å¼ºå¤§çš„ **Transformers ç”Ÿæ€ä½“ç³»**ã€‚\n",
    "\n",
    "### ğŸš€ Hugging Face æä¾›çš„æ ¸å¿ƒå†…å®¹\n",
    "\n",
    "| åŠŸèƒ½ | æè¿° |\n",
    "|------|------|\n",
    "| **Model Hub** | å¼€æ”¾å¼æ¨¡å‹åº“ï¼Œå¯ä¸‹è½½æˆ–åˆ†äº«é¢„è®­ç»ƒæ¨¡å‹ï¼ˆGPTã€BERTã€LLaMAã€Diffusionç­‰ï¼‰ |\n",
    "| **Transformersåº“** | ç»Ÿä¸€æ¥å£ï¼Œä¾¿äºå¿«é€ŸåŠ è½½ä¸»æµæ¨¡å‹ |\n",
    "| **Datasets** | ç®¡ç†å’Œå¤„ç†å¤§è§„æ¨¡æ•°æ®é›† |\n",
    "| **Tokenizers** | é«˜æ€§èƒ½åˆ†è¯å™¨ |\n",
    "| **Diffusers** | æ‰©æ•£æ¨¡å‹åº“ï¼ˆæ”¯æŒæ–‡ç”Ÿå›¾ã€å›¾ç”Ÿå›¾ï¼‰ |\n",
    "| **Spaces** | åœ¨çº¿éƒ¨ç½²ä¸åˆ†äº« ML åº”ç”¨çš„å¹³å°ï¼ˆåŸºäº Gradio / Streamlitï¼‰ |\n",
    "| **AutoTrain** | æ— ä»£ç è®­ç»ƒæ¨¡å‹ |\n",
    "\n",
    "### âœ¨ å¯æ‰§è¡Œç¤ºä¾‹ä»£ç \n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "print(generator(\"Hello\"))\n",
    "```\n",
    "\n",
    "##  ä»€ä¹ˆæ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼Ÿ\n",
    "\n",
    "**å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal Language Modelï¼ŒCLMï¼‰** æ˜¯ä¸€ç§åªèƒ½åˆ©ç”¨ **è¿‡å»å·²å‡ºç°çš„æ–‡æœ¬ä¿¡æ¯** æ¥é¢„æµ‹ **ä¸‹ä¸€ä¸ª token** çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚  \n",
    "å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸¥æ ¼éµå¾ª **æ—¶é—´å› æœé¡ºåº**ï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥å†…å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db47d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae6ffe1fb284ccbaddf2b7ae0edb2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c8dd55acb544c38917529be597dceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa97864f7844c96b1c98c847edb8860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "# ä½¿ç”¨AutoModelForCausalLMè‡ªåŠ¨åŠ è½½ä¸€ä¸ªå› æœè¯­è¨€æ¨¡å‹ï¼ˆcausal language modelï¼‰\n",
    "# from_pretrainedæ–¹æ³•ç”¨äºä»Hugging Face HubåŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",# æŒ‡å®šæ¨¡å‹çš„åç§°\n",
    "    device_map=\"auto\",# è‡ªåŠ¨åˆ†é…æ¨¡å‹åˆ°å¯ç”¨çš„è®¾å¤‡ï¼ˆå¦‚GPUï¼‰\n",
    "    torch_dtype=\"auto\",# è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„PyTorchæ•°æ®ç±»å‹\n",
    "    trust_remote_code=True,# å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“ä¸­è‡ªå®šä¹‰çš„ Python ä»£ç ï¼ˆæŸäº›æ¨¡å‹ç»“æ„ä¸æ˜¯æ ‡å‡†çš„ï¼‰ï¼›è¿™ä¸ªè®¾ç½®éå¸¸å…³é”®ï¼Œå› ä¸ºæœ‰äº›æ¨¡å‹ä¸æ˜¯ç”¨transformersçš„æ ‡å‡†ç»“æ„å†™çš„ï¼ˆå¦‚Qwenï¼‰ï¼Œè€Œæ˜¯æä¾›äº†è‡ªå®šä¹‰ç±»ï¼Œè¿™ä¸€å‚æ•°å…è®¸ä»è¿œç¨‹åŠ è½½è¿™äº›å®šä¹‰\n",
    "\n",
    ")\n",
    "\n",
    "# æ‰“å°æ¨¡å‹çš„ç»“æ„ï¼ˆå®é™…ä¸Šæ—¶pyTorchæ¨¡å‹çš„ç»“æ„ï¼‰\n",
    "# å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„æ¨¡å—ç»„æˆã€å±‚æ•°ã€å‚æ•°æ•°é‡ç­‰ä¿¡æ¯\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3d697",
   "metadata": {},
   "source": [
    "## æ•´ä½“ç»“æ„\n",
    "`Qwen2ForCausalLM`æ˜¯ä¸€ä¸ªå› æœè¯­è¨€æ¨¡å‹ï¼Œå±äºDecoder-only Transformeræ¶æ„ï¼Œç”¨äºè‡ªå›å½’ç”Ÿæˆä»»åŠ¡ã€‚\n",
    "ç»“æ„æ¦‚è§ˆï¼š\n",
    "```rust\n",
    "è¾“å…¥ â†’ è¯åµŒå…¥ Embedding â†’ 24å±‚DecoderLayer â†’ RMSNorm â†’ lm_head â†’ è¾“å‡º logits \n",
    "```\n",
    "### ä¸»è¦ç»„ä»¶è§£æ\n",
    "1. `Embedding(151936, 896)`\n",
    "     - è¯è¡¨å¤§å°ï¼š151936\n",
    "          - åœ¨ NLP æ¨¡å‹ä¸­ï¼Œæ–‡æœ¬ä¼šè¢«åˆ‡åˆ†æˆ tokensï¼ˆè¯å…ƒï¼‰ï¼Œæ¯ä¸ª token éƒ½ä¼šå¯¹åº”ä¸€ä¸ªè¯è¡¨ä¸­çš„ç¼–å·ï¼ˆidï¼‰ã€‚è¯è¡¨çš„å¤§å°å°±æ˜¯æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«çš„ ä¸åŒ token çš„æ•°é‡æ€»å’Œã€‚\n",
    "     - æ¯ä¸ªtokenç”¨896ç»´å‘é‡è¡¨ç¤º\n",
    "          - æ¨¡å‹ä¸€å…±æ”¯æŒ 151,936 ä¸ªå¯èƒ½çš„ tokenï¼Œæ¯ä¸ª token è¢«æ˜ å°„æˆä¸€ä¸ª 896 ç»´å‘é‡ã€‚\n",
    "2. `ModuleList(24 x Qwen2DecoderLayer)`\n",
    "\n",
    "     æ¨¡å‹åŒ…å«24å±‚Transformer Decoderå±‚ï¼Œæ¯å±‚ç»“æ„å¦‚ä¸‹ï¼š\n",
    "     ```css\n",
    "     Input\n",
    "     â””â”€â”€ RMSNorm\n",
    "          â””â”€â”€ Self Attention\n",
    "     â””â”€â”€ RMSNorm\n",
    "          â””â”€â”€ MLP (å‰é¦ˆç½‘ç»œ)\n",
    "     ```\n",
    "3. `Qwen2Attention`ï¼ˆè‡ªæ³¨æ„åŠ›å±‚ï¼‰\n",
    "     ```scss\n",
    "     (q_proj):  Linear(896 -> 896)\n",
    "     (k_proj):  Linear(896 -> 128)\n",
    "     (v_proj):  Linear(896 -> 128)\n",
    "     (o_proj):  Linear(896 -> 896)\n",
    "     ```\n",
    "     è¯´æ˜ï¼š\n",
    "     - Qã€Kã€V ç»´åº¦ä¸åŒï¼ˆä¸æ˜¯å¸¸è§çš„ç­‰ç»´åº¦ï¼‰\n",
    "     - attention head çš„ key/value ç»´åº¦è¾ƒå°ï¼ˆ128ï¼‰ï¼Œå¯èŠ‚çœæ˜¾å­˜å¹¶æé«˜é€Ÿåº¦\n",
    "     - ä½¿ç”¨ RoPE æ—‹è½¬ä½ç½®ç¼–ç  (rotary_emb)\n",
    "          - è¿™ç§ç»“æ„å±äº Multi-Query Attention (MQA) æˆ– Grouped-Query Attention çš„å˜ä½“ï¼Œå³ä¸åŒ head å…±äº« key/valueï¼Œæœ‰åˆ©äºæ¨ç†åŠ é€Ÿã€‚\n",
    "\n",
    "     è§£é‡Š:\n",
    "     - Transformerï¼ˆåŒ…æ‹¬ Qwen2ã€ChatGPT ç­‰ï¼‰åœ¨ç†è§£æ–‡æœ¬æ—¶ï¼Œä¾é ä¸€ç§å« è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf Attentionï¼‰ çš„æŠ€æœ¯ã€‚å¯ä»¥ç®€å•ç†è§£ä¸ºæ¨¡å‹åœ¨è¯»ä¸€å¥è¯æ—¶ï¼Œä¼šâ€œæ€è€ƒâ€æ¯ä¸ªè¯å’Œå…¶ä»–è¯ä¹‹é—´çš„å…³ç³»ã€‚\n",
    "     - Attentionä¸­çš„Qã€Kã€Væ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "          | åç§°           | åƒä»€ä¹ˆ   | ä½œç”¨              |\n",
    "          | ------------ | ----- | --------------- |\n",
    "          | **Qï¼ˆQueryï¼‰** | æé—®é¢˜çš„äºº | æˆ‘è¦å…³æ³¨è°ï¼Ÿ          |\n",
    "          | **Kï¼ˆKeyï¼‰**   | å…³é”®è¯   | æˆ‘æ˜¯ä¸æ˜¯ä½ è¦æ‰¾çš„äººï¼Ÿ      |\n",
    "          | **Vï¼ˆValueï¼‰** | å…·ä½“ä¿¡æ¯  | å¦‚æœå…³æ³¨æˆ‘ï¼Œæˆ‘èƒ½æä¾›ä»€ä¹ˆå†…å®¹ï¼Ÿ |\n",
    "          \n",
    "          ç®€å•æ¯”å–»ï¼š\n",
    "          > ä½ åˆ°å›¾ä¹¦é¦†æ‰¾ä¸€æœ¬æœ‰å…³ AI çš„ä¹¦\n",
    "          > - ä½ çš„éœ€æ±‚å°±æ˜¯ **Q**\n",
    "          > - æ¯æœ¬ä¹¦çš„æ ‡ç­¾æ˜¯ **K**ï¼ˆæ¯”å¦‚ï¼šAIã€å†å²ã€å“²å­¦â€¦ï¼‰\n",
    "          > - K åŒ¹é…åï¼Œä½ çœŸæ­£é˜…è¯»åˆ°çš„å†…å®¹å°±æ˜¯ **V**\n",
    "          \n",
    "          ç³»ç»Ÿæ ¹æ® Q å’Œ K çš„åŒ¹é…ç¨‹åº¦å†³å®š **å“ªäº›è¯åº”è¯¥é‡ç‚¹å…³æ³¨**ï¼Œç„¶åå†å»è¯»å–å¯¹åº” Vã€‚\n",
    "     - `(q_proj): Linear(896 -> 896)`\n",
    "\n",
    "          è¿™æ˜¯ä¸€ä¸ªçº¿æ€§æ˜ å°„å±‚ï¼Œä½œç”¨æ˜¯æŠŠè¾“å…¥çš„æ•°æ®ï¼ˆè¯å‘é‡ï¼‰è½¬æ¢æˆå¯¹åº”çš„Qã€Kã€V\n",
    "\n",
    "          | åç§°                   | å«ä¹‰                  |\n",
    "          | -------------------- | ------------------- |\n",
    "          | `Linear(896 -> 896)` | è¾“å…¥å‘é‡ç»´åº¦ 896ï¼Œè¾“å‡ºä¹Ÿä¸º 896 |\n",
    "          | `Linear(896 -> 128)` | ç¼©å°ç»´åº¦åˆ° 128ï¼Œæé«˜é€Ÿåº¦ã€é™ä½æ˜¾å­˜ |\n",
    "\n",
    "          å› ä¸ºQéœ€è¦è¡¨è¾¾æ›´å¤æ‚çš„éœ€æ±‚ï¼Œéœ€è¦ä¿æŒé«˜ç»´åº¦ï¼Œè€ŒK/Vä¸»è¦ç”¨äºåŒ¹é…ï¼Œä¸éœ€è¦å¤ªå¤§ç»´åº¦ã€‚è¿™æ ·åšå¯ä»¥\n",
    "          - å‡å°‘è®¡ç®—\n",
    "          - åŠ å¿«ç”Ÿæˆé€Ÿåº¦\n",
    "          - é™ä½ç°å­˜æ¶ˆè€—\n",
    "\n",
    "     - `Multi-Query Attention (MQA)`\n",
    "\n",
    "          ä¼ ç»Ÿçš„ Attention æ¯ä¸ªå¤´éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„ Qã€Kã€Vï¼Œå¤šå¤´ç»“æ„ä¾‹å¦‚ï¼š\n",
    "          ```scss\n",
    "          head1(Q,K,V)\n",
    "          head2(Q,K,V)\n",
    "          ...\n",
    "          headN(Q,K,V)\n",
    "          ```\n",
    "          è€ŒMQAçš„åšæ³•æ˜¯ï¼šå¤šä¸ªå¤´å…±äº«Kã€Vï¼Œåªä¿ç•™ä¸åŒçš„Q\n",
    "\n",
    "          ç±»æ¯”ï¼š\n",
    "          > ä¸€ä¸ªå…¬å¸å¤šä¸ªéƒ¨é—¨ï¼ˆä¸åŒ headsï¼‰éƒ½è¦æŸ¥åŒä¸€ä¸ªæ•°æ®åº“ï¼ˆå…±äº« K/Vï¼‰ï¼Œä¸ç”¨ä¸ºæ¯ä¸ªéƒ¨é—¨å¤åˆ¶ä¸€ä»½æ•°æ®åº“ â†’ èŠ‚çœèµ„æºã€æŸ¥è¯¢æ›´å¿«ã€‚\n",
    "\n",
    "     - RoPE æ—‹è½¬ä½ç½®ç¼–ç \n",
    "\n",
    "          å› ä¸ºæ¨¡å‹æœ¬èº«ä¸çŸ¥é“è¯çš„é¡ºåºï¼Œéœ€è¦ä½ç½®ç¼–ç å¸®åŠ©å®ƒç†è§£ã€‚\n",
    "\n",
    "          RoPEï¼ˆRotary Position Embeddingï¼‰æ˜¯ä¸€ç§æ›´å…ˆè¿›çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œè®©æ¨¡å‹ï¼š\n",
    "          - æ›´çµæ´»ç†è§£è¿œè·ç¦»è¯è¯­å…³ç³»\n",
    "          - æ›´é€‚åˆé•¿æ–‡æœ¬\n",
    "4. `Qwen2MLP`ï¼ˆå‰é¦ˆç½‘ç»œå±‚ï¼‰\n",
    "     ```yaml\n",
    "     gate_proj: Linear(896 -> 4864)\n",
    "     up_proj:   Linear(896 -> 4864)\n",
    "     down_proj: Linear(4864 -> 896)\n",
    "     (act_fn):  SiLU æ¿€æ´»\n",
    "     ```\n",
    "     \n",
    "     è§£é‡Šï¼š\n",
    "\n",
    "     - MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰æ˜¯ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œåœ¨æ¯å±‚Transformerä¸­è´Ÿè´£ç†è§£å’Œæå–æ›´å¤æ‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚\n",
    "\n",
    "          >æ¨¡å‹çš„Attentionè´Ÿè´£ç†è§£è¯ä¹‹é—´çš„å…³ç³»ï¼›\n",
    "          >\n",
    "          >MLPåˆ™è´Ÿè´£æ›´æ·±å±‚ç†è§£è¯æœ¬èº«çš„å«ä¹‰ã€‚\n",
    "\n",
    "     - MLPå¤„ç†ä¸€ä¸ªå‘é‡ï¼ˆç»´åº¦896ï¼‰ï¼Œç„¶åæ‰©å±•åˆ°æ›´å¤§çš„ç»´åº¦ï¼ˆ4864ï¼‰,å†ç¼©å›å»ã€‚\n",
    "\n",
    "          >è¾“å…¥ï¼š896ç»´çš„tokenè¡¨ç¤º\n",
    "          >\n",
    "          >->æ‰©å¼ åˆ°4864ç»´ï¼ˆæ‹†åˆ†æˆæ›´å¤šç‰¹å¾æ¥ç†è§£ï¼‰\n",
    "          >\n",
    "          >->å‹ç¼©å›896ç»´ï¼Œå°†æç‚¼åçš„ä¿¡æ¯è¿”å›\n",
    "     - `Gated MLP`\n",
    "          é—¨æ§æœºåˆ¶ â†’ åƒé—¸é—¨ä¸€æ ·æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼Œè®©æ¨¡å‹å­¦ä¹ â€œå“ªäº›ä¿¡æ¯é‡è¦ï¼Œå“ªäº›è¦ä¸¢æ‰â€\n",
    "\n",
    "          ä¼˜ç‚¹ï¼š\n",
    "          - æ›´èªæ˜\n",
    "          - æ›´çœå‚æ•°\n",
    "          - è¡¨è¾¾èƒ½åŠ›æ›´å¼º\n",
    "          - å¤§æ¨¡å‹è®­ç»ƒæ›´ç¨³å®š\n",
    "5. `Qwen2RMSNorm`\n",
    "- RMSNorm æ˜¯ä¸€ç§ å½’ä¸€åŒ–æ–¹æ³•ï¼Œç”¨äºè®©è®­ç»ƒæ›´ç¨³å®šã€‚\n",
    "- å½’ä¸€åŒ–çš„ä½œç”¨ï¼š\n",
    "     - è°ƒæ•´æ•°æ®èŒƒå›´ï¼Œé¿å…çˆ†ç‚¸æˆ–æ¶ˆå¤±\n",
    "     - è®©è®­ç»ƒæ›´å®¹æ˜“æ”¶æ•›\n",
    "     - æå‡æ•ˆæœ\n",
    "\n",
    "     | LayerNorm | RMSNorm     |\n",
    "     | --------- | ----------- |\n",
    "     | åŒæ—¶è€ƒè™‘å‡å€¼å’Œæ–¹å·® | åªè€ƒè™‘æ–¹å·®ï¼ˆå¹³æ–¹å‡å€¼ï¼‰ |\n",
    "     | è®¡ç®—æ›´é‡      | æ›´è½»ï¼Œæ›´é€‚åˆå¤§æ¨¡å‹   |\n",
    "     | ç¨³å®šæ€§ç¨å·®     | æ›´ç¨³å®š         |\n",
    "\n",
    "6. `lm_head: Linear(896 â†’ 151936)`\n",
    "- æœ€åè¾“å‡º logitsï¼Œç”¨äº softmax é¢„æµ‹ä¸‹ä¸€ä¸ª token\n",
    "     - è¾“å…¥ï¼šæ¨¡å‹å†…éƒ¨896ç»´è¡¨ç¤º\n",
    "     - è¾“å‡ºï¼šä¸€ä¸ªåŒ…å«151936ä¸ªæ•°å­—çš„å‘é‡\n",
    "     - æ¯ä¸ªæ•°å­—è¡¨ç¤ºä¸€ä¸ªè¯çš„æ¦‚ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25be9b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä»hugging face model hubåŠ è½½ä¸æŒ‡å®šæ¨¡å‹é…å¥—çš„åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "# è¿”å›åˆ†è¯å™¨ä¸­å®šä¹‰çš„æ‰€æœ‰ç‰¹æ®ŠtokenåŠå…¶å«ä¹‰\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b852fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, create by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "è®²ä¸€ä¸ªçŒ«æœ‰å…³çš„ç¬‘è¯<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "========================================\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   1855,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  99526,  46944, 100472,\n",
      "         101063,   9370, 109959, 151645,    198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# æ„å»ºpromptå’Œtokenizer\n",
    "\n",
    "prompt = \"è®²ä¸€ä¸ªçŒ«æœ‰å…³çš„ç¬‘è¯\"\n",
    "# ä½¿ç”¨ Qwen çš„å¤šè½®å¯¹è¯ç»“æ„ï¼Œæ„å»ºä¸€æ¡å®Œæ•´çš„æ¶ˆæ¯åºåˆ—\n",
    "message = [\n",
    "    {\"role\":\"system\",\"content\":\"You are Qwen, create by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\":\"user\",\"content\":prompt}\n",
    "]\n",
    "\n",
    "# åº”ç”¨ Qwen ä¸“ç”¨çš„èŠå¤©æ¨¡æ¿ï¼Œå°†ç»“æ„åŒ–çš„messagesè½¬æ¢æˆâ€œæ¨¡å‹é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„åŸå§‹æ–‡æœ¬æ ¼å¼â€\n",
    "text = tokenizer.apply_chat_template(\n",
    "    message, # ä¼ å…¥å¯¹è¯æ¶ˆæ¯åˆ—è¡¨\n",
    "    tokenize = False, # ä¸è¿›è¡Œåˆ†è¯ï¼Œåªè¿”å›æ‹¼æ¥åçš„çº¯å­—ç¬¦ä¸²ã€‚å³å¾—åˆ°çš„textæ˜¯strï¼Œè€Œä¸æ˜¯token ids\n",
    "    add_generation_prompt = True # åœ¨æœ«å°¾åŠ ä¸Š assistant çš„èµ·å§‹æ ‡è®°ï¼Œå¼•å¯¼æ¨¡å‹å¼€å§‹â€œç”Ÿæˆå›ç­”â€ï¼Œå³å‘Šè¯‰æ¨¡å‹â€œç°åœ¨è½®åˆ°ä½ è¯´è¯äº†â€\n",
    ")\n",
    "\n",
    "# å°†æ‹¼æ¥åçš„æ–‡æœ¬ç¼–ç æˆæ¨¡å‹å¯æ¥å—çš„å¼ é‡(åˆ†è¯ã€æ˜ å°„ã€å¼ é‡åŒ–)\n",
    "model_inputs = tokenizer(\n",
    "    [text],# ç”¨åˆ—è¡¨æ˜¯å› ä¸ºtokenizeré»˜è®¤æ”¯æŒbatchï¼Œæ–¹ä¾¿ä»¥åä¸€æ¬¡æ¨ç†å¤šæ¡è¾“å…¥\n",
    "    return_tensors=\"pt\" # è¿”å›PyTorchçš„å¼ é‡æ ¼å¼\n",
    ").to(model.device)\n",
    "\n",
    "print(text)\n",
    "print(\"====\" * 10)\n",
    "# æ‰“å°input_idså’Œattention_mask\n",
    "## input_idsè¡¨ç¤ºæ¯ä¸ªtokençš„id\n",
    "## attention_maskç”¨äºæŒ‡ç¤ºæ¨¡å‹å“ªäº›tokenæ˜¯æœ‰æ•ˆçš„\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e40778",
   "metadata": {},
   "source": [
    "## model_inputså­—æ®µè§£é‡Š\n",
    "- input_ids\n",
    "  æ¯ä¸ªæ•°å­—å¯¹åº”ä¸€ä¸ªtokenï¼Œè¿™äº›æ•°å­—ï¼ˆidï¼‰æ¥è‡ªtokenizerçš„è¯è¡¨\n",
    "- attention_mask\n",
    "  1è¡¨ç¤ºæœ‰æ•ˆï¼Œ0è¡¨ç¤ºå¡«å……ï¼Œè¿™é‡Œä¸€èˆ¬å…¨æ˜¯1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9037de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“ç„¶ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå…³äºçŒ«çš„æœ‰è¶£ç¬‘è¯ï¼š\n",
      "\n",
      "ä¸ºä»€ä¹ˆçŒ«å’ªæ€»æ˜¯åœ¨ç¡è§‰ï¼Ÿå› ä¸ºä»–ä»¬çŸ¥é“â€œå¤œâ€å°±æ˜¯â€œç¡â€ã€‚\n",
      "\n",
      "è¿™ä¸ªå°å¹½é»˜æ¥è‡ªä¸­æ–‡è°šè¯­ï¼Œâ€œå¤œçŒ«å­â€ï¼ŒåŸæ„æ˜¯è¯´é‚£äº›åœ¨å¤œæ™šå‡ºæ¥è§…é£Ÿã€ç©è€æˆ–å‡ºæ²¡çš„äººã€‚è¿™é‡Œçš„â€œå¤œâ€å­—ä¸çŒ«çš„è¡Œä¸ºç›¸ä¼¼ï¼Œéƒ½è¡¨ç¤ºå¤œæ™šæˆ–æ™šä¸Šã€‚æ‰€ä»¥ï¼Œæ— è®ºä½•æ—¶ä½•åœ°ï¼Œå¦‚æœçœ‹åˆ°ä¸€åªçŒ«åœ¨å¤œé—´æ´»åŠ¨ï¼Œäººä»¬éƒ½ä¼šè§‰å¾—å®ƒåƒæ˜¯å¤œè¡Œæ€§çš„åŠ¨ç‰©ï¼Œå› ä¸ºå®ƒä»¬ç¡®å®ä¼šæ„Ÿåˆ°å›°å€¦å’Œéœ€è¦ä¼‘æ¯ã€‚\n",
      "\n",
      "è¿™ä¸ªç¬‘è¯é€šè¿‡å·§å¦™çš„æ¯”å–»å’Œå¯¹ç”Ÿæ´»ç»†èŠ‚çš„æ·±åˆ»æ´å¯Ÿï¼Œè®©è§‚ä¼—èƒ½å¤Ÿè½»æ¾åœ°ç¬‘å‡ºå£°æ¥ï¼ŒåŒæ—¶ä¹Ÿå¸¦æœ‰ä¸€ä¸æ™ºæ…§å’Œè¶£å‘³æ€§ã€‚å¸Œæœ›è¿™ä¸ªç¬‘è¯èƒ½ä¸ºæ‚¨çš„èŠå¤©å¢æ·»ä¸å°‘æ¬¢ä¹ï¼\n"
     ]
    }
   ],
   "source": [
    "# æ¨ç†å’Œè§£ç \n",
    "\n",
    "# æ¨ç†\n",
    "## generate()æ˜¯hugging faceçš„è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆæ¥å£ï¼Œä¼šæ ¹æ®è¾“å…¥çš„input_idsç”Ÿæˆæ–°çš„tokenåºåˆ—\n",
    "## é»˜è®¤æ˜¯è´ªå©ªè§£ç ï¼ˆå³æ¯ä¸€æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼‰\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs, # tokenizerç¼–ç åçš„è¾“å…¥\n",
    "    max_new_tokens=512 # æœ€å¤šç”Ÿæˆ512ä¸ªæ–°token\n",
    ")\n",
    "\n",
    "# æˆªå–ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)# ä»ç”Ÿæˆç»“æœä¸­å»æ‰åŸå§‹promptéƒ¨åˆ†ï¼Œä»…ä¿ç•™æ–°ç”Ÿæˆçš„token\n",
    "]\n",
    "\n",
    "# è§£ç ï¼šå°†ç”Ÿæˆçš„token idsè½¬æˆå¯è¯»çš„æ–‡æœ¬\n",
    "response = tokenizer.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True # å»é™¤ç‰¹æ®Šæ ‡è®°ï¼Œå¦‚ <|im_start|>ã€<|im_end|> ç­‰\n",
    ")[0]# å–ç¬¬ä¸€æ¡ç»“æœï¼ˆå› ä¸ºè¿™é‡Œåªæœ‰ä¸€æ¡è¾“å…¥ï¼‰\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0364ee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åœ¨ä½ é¢å‰ï¼Œæœ‰ä¸€åªçŒ«ã€‚å®ƒçœ‹èµ·æ¥å¾ˆå¹³é™ï¼Œä½†å…¶å®å†…å¿ƒå´å……æ»¡äº†ä¸å®‰ã€‚\n",
      "æœ‰ä¸€å¤©ï¼Œè¿™åªçŒ«çªç„¶å¯¹ä½ è¯´ï¼šâ€œä½ çŸ¥é“å—ï¼Ÿæˆ‘æœ€è¿‘ä¸€ç›´åœ¨æƒ³ä¸€ä¸ªé—®é¢˜ã€‚â€\n",
      "â€œä»€ä¹ˆé—®é¢˜ï¼Ÿâ€\n",
      "â€œæˆ‘æƒ³çŸ¥é“ï¼Œå¦‚æœä¸€åªçŒ«æƒ³è¦æˆä¸ºä¸€åªçœŸæ­£çš„çŒ«ï¼Œå®ƒä¼šæ€ä¹ˆåšå‘¢ï¼Ÿâ€\n",
      "â€œè¿™å¬èµ·æ¥åƒæ˜¯ä¸ªå¥½é—®é¢˜ï¼Œâ€ä½ å›ç­”è¯´ï¼Œâ€œä½†æ˜¯ï¼Œå¦‚æœä½ é—®é‚£åªçŒ«ï¼Œå®ƒå¯èƒ½ä¼šå‘Šè¯‰ä½ ï¼Œå®ƒæƒ³è¦å˜æˆä¸€åªç‹—ï¼Œç„¶ååˆå˜æˆäº†ä¸€ä¸ªçŒ«ï¼Œç„¶ååˆå˜æˆäº†ä¸€ä¸ªç‹—â€¦â€¦â€\n",
      "â€œé‚£çœŸæ˜¯å¤ªæœ‰è¶£äº†ï¼â€çŒ«è¯´ï¼Œâ€œä¸è¿‡ï¼Œæˆ‘æ›´æƒ³çŸ¥é“ï¼Œå¦‚æœä¸€åªçŒ«çœŸçš„æƒ³è¦å˜æˆä¸€åªçœŸæ­£çš„çŒ«ï¼Œå®ƒä¼šæ€ä¹ˆåšå‘¢ï¼Ÿâ€\n",
      "â€œæˆ‘ä¼šä¸€ç›´ä¿æŒå®ƒçš„åŸå§‹çŠ¶æ€ï¼Œç›´åˆ°å®ƒæ„Ÿåˆ°æ»¡è¶³ä¸ºæ­¢ã€‚â€çŒ«å›ç­”é“ã€‚â€œæˆ–è€…ï¼Œæˆ‘å¯èƒ½ä¼šè®©å®ƒå˜æˆä¸€åªç‹—ï¼Œç„¶åå˜æˆä¸€åªçŒ«ï¼Œç„¶åå˜æˆä¸€åªç‹—â€¦â€¦â€\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨transformersçš„pipelineç®€åŒ–æµç¨‹\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1.ç”Ÿæˆpipeline\n",
    "## åˆ›å»ºæ–‡æœ¬ç”Ÿæˆpipeline(æµæ°´çº¿)å¯¹è±¡\n",
    "generator = pipeline(\n",
    "    \"text-generation\", # æŒ‡å®šä»»åŠ¡ç±»å‹ä¸ºæ–‡æœ¬ç”Ÿæˆ\n",
    "    model = model, # æŒ‡å®šè¦ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "    tokenizer = tokenizer, # æŒ‡å®šå¯¹åº”çš„tokenizer(åˆ†è¯å™¨),ç”¨äºç¼–ç è¾“å…¥å’Œè§£ç è¾“å‡º\n",
    "    return_full_text = False, # åªè¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬éƒ¨åˆ†ï¼Œä¸åŒ…å«åŸå§‹æç¤ºè¯\n",
    "    max_new_tokens = 500,# é™åˆ¶æœ€å¤šç”Ÿæˆçš„tokenæ•°é‡ï¼ˆæ–°ç”Ÿæˆï¼Œä¸åŒ…å«æç¤ºè¯ï¼‰\n",
    "    do_sample = False # ä¸è¿›è¡Œé‡‡æ ·ï¼ˆå³ä½¿ç”¨è´ªå©ªè§£ç ï¼‰ï¼Œä¿æŒç¨³å®šæ€§è¾“å‡º\n",
    ")\n",
    "\n",
    "# 2.æ„å»ºprompt\n",
    "## pipelineä¼šè‡ªåŠ¨ä½¿ç”¨tokenizerçš„chatæ¨¡æ¿æ¥å¤„ç†æ¶ˆæ¯\n",
    "messages = [\n",
    "    {'role':\"user\",\"content\":\"å†™ä¸€ä¸ªå’ŒçŒ«æœ‰å…³çš„ç¬‘è¯\"}\n",
    "]\n",
    "\n",
    "# 3.è¾“å‡ºå¹¶è§£ç \n",
    "output = generator(messages) # è°ƒç”¨pipelineå®Œæˆè‡ªåŠ¨ç¼–ç ã€æ¨ç†å’Œè§£ç \n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14bc1de",
   "metadata": {},
   "source": [
    "## é‡‡æ ·\n",
    "é‡‡æ ·æ˜¯æŒ‡ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œä»é¢„æµ‹çš„è¯æ±‡æ¦‚ç‡åˆ†å¸ƒä¸­éšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªè¯ï¼Œè€Œä¸æ˜¯æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ã€‚è¿™æ˜¯ç”Ÿæˆå¼æ¨¡å‹ä¸­å¸¸ç”¨çš„ä¸€ç§éç¡®å®šæ€§è§£ç ç­–ç•¥ï¼Œèƒ½è®©è¾“å‡ºæ›´åŠ å¤šæ ·è‡ªç„¶ã€‚\n",
    "- do_sample=Falseæ—¶ï¼ˆä¸é‡‡æ ·ï¼‰\n",
    "- ä½¿ç”¨è´ªå©ªè§£ç ï¼ˆGreedy decodingï¼‰ï¼šæ¯ä¸€æ­¥éƒ½é€‰æ‹©å½“å‰æ¦‚ç‡æœ€é«˜çš„è¯ã€‚\n",
    "- æˆ–è€…ä½¿ç”¨ Beam Searchï¼ˆæŸæœç´¢ï¼‰ï¼šæ¢ç´¢å¤šä¸ªæ¦‚ç‡é«˜çš„è·¯å¾„ã€‚\n",
    "- ç»“æœæ˜¯ç¡®å®šæ€§çš„ â€”â€” æ¯æ¬¡è¾“å…¥ç›¸åŒçš„ promptï¼Œè¾“å‡ºå®Œå…¨ä¸€æ ·ã€‚\n",
    "- do_sample=Trueæ—¶ï¼ˆå¯ç”¨é‡‡æ ·ï¼‰\n",
    "  - ä»æ¨¡å‹ç»™å‡ºçš„è¯æ±‡æ¦‚ç‡åˆ†å¸ƒä¸­ï¼ŒæŒ‰æ¦‚ç‡éšæœºé‡‡æ ·ä¸€ä¸ªè¯ã€‚\n",
    "  - å¸¸é…åˆä»¥ä¸‹å‚æ•°\n",
    "  - temperatureï¼šæ§åˆ¶éšæœºæ€§çš„ç¨‹åº¦ï¼ˆé«˜æ¸©â†’æ›´éšæœºï¼‰\n",
    "  - top_kï¼šä»…ä»æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯ä¸­é‡‡æ ·\n",
    "  - top_p(nucleus sampling)ï¼šä»…ä»ç´¯è®¡æ¦‚ç‡è¶…è¿‡pçš„è¯ä¸­é‡‡æ ·\n",
    "  - ç»“æœæ˜¯éç¡®å®šæ€§çš„ â€”â€” ç›¸åŒè¾“å…¥ï¼Œæ¯æ¬¡è¾“å‡ºå¯èƒ½ä¸åŒï¼Œæ›´å…·åˆ›æ„ä½†ä¹Ÿå¯èƒ½åç¦»ä¸»é¢˜ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
