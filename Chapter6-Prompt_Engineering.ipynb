{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66a59ae",
   "metadata": {},
   "source": [
    "# Chapter 6 - Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc19f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b5318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/openbayes/home/huggingface\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec4fa2",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb478575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b32df308784cdda2e75d7766c8d665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603484a8546947df9681f6d42357a88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c6c62c65ea4e98a25d879f943357fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df58e3f60b34cda8c90d813d2c1f25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd5e23b129f4e32bbce43f2f667520d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923b56e8a7ff4d5fad6caadc0ab2c46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34739924841f450093158e1cd77eca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34a6af734ed4801ae376b459e8de861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Passing `generation_config` together with generation-related arguments=({'do_sample', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "# 导入依赖\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",  \n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,\n",
    "    return_full_text=False,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d5a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以，以下是一个关于猫的有趣笑话：\n",
      "\n",
      "为什么猫咪总是喜欢在窗台上晒太阳？\n",
      "因为它们知道，这是它们最喜欢的“日光浴”！\n"
     ]
    }
   ],
   "source": [
    "# 创建prompt\n",
    "message = [\n",
    "    {\"role\":\"user\", \"content\":\"请编写一个跟猫有关的笑话\"}\n",
    "]\n",
    "\n",
    "# 生成输出\n",
    "output = pipe(message)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b6e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "请编写一个跟猫有关的笑话<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 应用提示词对话模\n",
    "prompt = pipe.tokenizer.apply_chat_template(message,tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a383886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'do_sample', 'temperature'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然，以下是一个与猫相关的有趣笑话：\n",
      "\n",
      "在一只名叫“毛毛”的猫咪眼中，有一只叫“小灰”的老鼠。一天，毛毛和小灰一起玩捉迷藏游戏，毛毛不小心把小灰藏起来了。小灰非常生气，说：“你这个笨蛋！我才是藏起来的，你怎么能让我来找呢？”\n",
      "\n",
      "毛毛很恼火，开始用爪子抓着小灰。小灰看着毛毛的样子，突然想起他平时喜欢听的故事《小熊理查德》里的故事。于是小灰说道：“哦，原来是这样啊。不过现在我可没有时间听你的故事了，我要去找吃的了。”\n",
      "\n",
      "毛毛被这只可爱的“小灰”吓得不敢靠近，只好乖乖地留在原地，等待着它的食物。\n",
      "\n",
      "这就是“毛毛与小灰”这一对小家伙之间的一个小小的幽默故事。希望这个笑话能够给各位带来一些欢乐！\n"
     ]
    }
   ],
   "source": [
    "# 使用一个高的temperature。温度越高，随机性越强\n",
    "output = pipe(message, do_sample = True, temperature = 1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa098b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在一片宁静的草地上，一只小猫咪正在悠闲地吃着新鲜的胡萝卜。突然，它看到了远处有只活泼的小狗正在追逐一只飞过的蝴蝶。小狗停下脚步，惊讶地看着那只小猫，问：“你怎么会在这里？”猫咪答道：“我只是享受我的独处时间。”\n",
      "\n",
      "小狗听了，开始用尖锐的声音叫喊：“别吃我！别吃我！”猫咪微笑着回答：“你看起来很有趣，但我更喜欢安静地吃胡萝卜。”\n",
      "\n",
      "小狗继续奔跑，而小猫则在一旁静静地看着，似乎也在享受这个短暂的平静时光。\n",
      "\n",
      "就这样，两人成为了彼此生活中的朋友，分享着一天的快乐和烦恼。这个故事告诉我们，有时候，简单的生活就是最好的调剂。\n"
     ]
    }
   ],
   "source": [
    "# 使用一个高的top_p\n",
    "output = pipe(message, do_sample = True, top_p = 1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3061444",
   "metadata": {},
   "source": [
    "# 高级提示词工程\n",
    "## 复杂提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e179ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"在上一篇文章中，我们探讨了注意力机制——现代深度学习模型中普遍采用的一种方法。注意力机制的概念有助于提升神经机器翻译应用的性能。在本文中，我们将介绍 Transformer 模型——一种利用注意力机制来提升模型训练速度的模型。Transformer 模型在某些特定任务上的表现优于谷歌神经机器翻译模型。然而，其最大的优势在于 Transformer 模型本身就非常适合并行化。事实上，谷歌云建议使用 Transformer 模型作为参考模型来使用其 Cloud TPU 服务。接下来，我们将深入剖析该模型，探究其工作原理。\n",
    "\n",
    "Transformer 模型最初在论文《Attention is All You Need》中提出。TensorFlow 实现了该模型，并将其包含在 Tensor2Tensor 包中。哈佛大学的自然语言处理小组编写了一份指南，用 PyTorch 实现标注了这篇论文。在这篇文章中，我们将尝试对概念进行一些简化，并逐一介绍，希望能帮助那些对该主题没有深入了解的人更容易理解。\n",
    "\n",
    "首先，让我们把模型看作一个黑盒。在机器翻译应用中，它会接收一个句子，并输出另一种语言的翻译。\n",
    "\n",
    "打开这个“黑盒”，我们可以看到编码组件、解码组件以及它们之间的连接。\n",
    "\n",
    "编码组件是一叠编码器（论文中将六个编码器堆叠在一起——数字六并没有什么特别之处，当然可以尝试其他排列方式）。解码组件是一叠相同数量的解码器。\n",
    "\n",
    "所有编码器的结构都相同（但它们的权重并不共享）。每个编码器又分为两个子层：\n",
    "\n",
    "编码器的输入首先流经一个自注意力层——该层帮助编码器在编码特定单词时关注输入句子中的其他单词。我们将在本文后面更详细地探讨自注意力机制。\n",
    "\n",
    "自注意力层的输出被送入一个前馈神经网络。同一个前馈网络被独立地应用于每个位置。\n",
    "\n",
    "解码器包含这两个层，但在它们之间有一个注意力层，它帮助解码器专注于输入句子的相关部分（类似于注意力在序列到序列模型中的作用）。\n",
    "\n",
    "现在我们已经了解了模型的主要组成部分，接下来让我们看看各种向量/张量以及它们如何在这些组件之间流动，从而将训练好的模型的输入转换为输出。\n",
    "\n",
    "与一般的自然语言处理应用一样，我们首先使用嵌入算法将每个输入词转换为一个向量。\n",
    "\n",
    "每个词都被嵌入到一个大小为 512 的向量中。我们将用这些简单的方框来表示这些向量。\n",
    "\n",
    "嵌入操作只发生在最底层的编码器中。所有编码器的共同抽象是，它们接收一个大小为 512 的向量列表——在最底层的编码器中，它是词嵌入；而在其他编码器中，它是其下方编码器的输出。该列表的大小是一个我们可以设置的超参数——它基本上等于训练数据集中最长句子的长度。\n",
    "\n",
    "在对输入序列中的单词进行嵌入后，每个单词都会依次经过编码器的两个子层。\n",
    "\n",
    "在这里，我们开始看到 Transformer 的一个关键特性：每个位置的单词在编码器中都有其自身的路径。自注意力层中的这些路径之间存在依赖关系。然而，前馈层不存在这些依赖关系，因此各个路径可以在经过前馈层时并行执行。\n",
    "\n",
    "接下来，我们将示例切换到一个更短的句子，并观察编码器每个子层中发生的情况。\n",
    "\n",
    "现在开始编码！\n",
    "\n",
    "正如我们之前提到的，编码器接收一个向量列表作为输入。它通过将这些向量传递给“自注意力”层来处理该列表，然后再传递给前馈神经网络，最后将输出发送到下一个编码器。\n",
    "\"\"\"\n",
    "\n",
    "# 提示词部分\n",
    "persona = \"你是大型语言模型的专家。你擅长将复杂的论文分解成易于理解的摘要。\\n\"\n",
    "instruction = \"总结所提供论文的关键发现.\\n\"\n",
    "context = \"你的总结应提取最关键的信息点，以帮助研究人员快速了解论文中最重要内容.\\n\"\n",
    "data_format = \"创建一个要点总结，概述方法。随后用一个简洁的段落概括主要结果。\\n\"\n",
    "audience = \"该摘要专为忙碌的研究人员设计，他们需要快速掌握大型语言模型的新趋势\\n\"\n",
    "tone = \"语气应专业且清晰。\\n\"\n",
    "\n",
    "data = f\"需要总结的文本:{text}\"\n",
    "\n",
    "query = persona + instruction + context + data_format + audience + tone + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be2d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "你是大型语言模型的专家。你擅长将复杂的论文分解成易于理解的摘要。\n",
      "总结所提供论文的关键发现.\n",
      "你的总结应提取最关键的信息点，以帮助研究人员快速了解论文中最重要内容.\n",
      "创建一个要点总结，概述方法。随后用一个简洁的段落概括主要结果。\n",
      "该摘要专为忙碌的研究人员设计，他们需要快速掌握大型语言模型的新趋势\n",
      "语气应专业且清晰。\n",
      "需要总结的文本:在上一篇文章中，我们探讨了注意力机制——现代深度学习模型中普遍采用的一种方法。注意力机制的概念有助于提升神经机器翻译应用的性能。在本文中，我们将介绍 Transformer 模型——一种利用注意力机制来提升模型训练速度的模型。Transformer 模型在某些特定任务上的表现优于谷歌神经机器翻译模型。然而，其最大的优势在于 Transformer 模型本身就非常适合并行化。事实上，谷歌云建议使用 Transformer 模型作为参考模型来使用其 Cloud TPU 服务。接下来，我们将深入剖析该模型，探究其工作原理。\n",
      "\n",
      "Transformer 模型最初在论文《Attention is All You Need》中提出。TensorFlow 实现了该模型，并将其包含在 Tensor2Tensor 包中。哈佛大学的自然语言处理小组编写了一份指南，用 PyTorch 实现标注了这篇论文。在这篇文章中，我们将尝试对概念进行一些简化，并逐一介绍，希望能帮助那些对该主题没有深入了解的人更容易理解。\n",
      "\n",
      "首先，让我们把模型看作一个黑盒。在机器翻译应用中，它会接收一个句子，并输出另一种语言的翻译。\n",
      "\n",
      "打开这个“黑盒”，我们可以看到编码组件、解码组件以及它们之间的连接。\n",
      "\n",
      "编码组件是一叠编码器（论文中将六个编码器堆叠在一起——数字六并没有什么特别之处，当然可以尝试其他排列方式）。解码组件是一叠相同数量的解码器。\n",
      "\n",
      "所有编码器的结构都相同（但它们的权重并不共享）。每个编码器又分为两个子层：\n",
      "\n",
      "编码器的输入首先流经一个自注意力层——该层帮助编码器在编码特定单词时关注输入句子中的其他单词。我们将在本文后面更详细地探讨自注意力机制。\n",
      "\n",
      "自注意力层的输出被送入一个前馈神经网络。同一个前馈网络被独立地应用于每个位置。\n",
      "\n",
      "解码器包含这两个层，但在它们之间有一个注意力层，它帮助解码器专注于输入句子的相关部分（类似于注意力在序列到序列模型中的作用）。\n",
      "\n",
      "现在我们已经了解了模型的主要组成部分，接下来让我们看看各种向量/张量以及它们如何在这些组件之间流动，从而将训练好的模型的输入转换为输出。\n",
      "\n",
      "与一般的自然语言处理应用一样，我们首先使用嵌入算法将每个输入词转换为一个向量。\n",
      "\n",
      "每个词都被嵌入到一个大小为 512 的向量中。我们将用这些简单的方框来表示这些向量。\n",
      "\n",
      "嵌入操作只发生在最底层的编码器中。所有编码器的共同抽象是，它们接收一个大小为 512 的向量列表——在最底层的编码器中，它是词嵌入；而在其他编码器中，它是其下方编码器的输出。该列表的大小是一个我们可以设置的超参数——它基本上等于训练数据集中最长句子的长度。\n",
      "\n",
      "在对输入序列中的单词进行嵌入后，每个单词都会依次经过编码器的两个子层。\n",
      "\n",
      "在这里，我们开始看到 Transformer 的一个关键特性：每个位置的单词在编码器中都有其自身的路径。自注意力层中的这些路径之间存在依赖关系。然而，前馈层不存在这些依赖关系，因此各个路径可以在经过前馈层时并行执行。\n",
      "\n",
      "接下来，我们将示例切换到一个更短的句子，并观察编码器每个子层中发生的情况。\n",
      "\n",
      "现在开始编码！\n",
      "\n",
      "正如我们之前提到的，编码器接收一个向量列表作为输入。它通过将这些向量传递给“自注意力”层来处理该列表，然后再传递给前馈神经网络，最后将输出发送到下一个编码器。\n",
      "<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\",\"content\":query}\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages, tokenize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0580be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在上一篇文章中，我们讨论了注意力机制在现代深度学习模型中的广泛应用，特别是如何提升神经机器翻译的应用性能。本文进一步介绍了Transformer模型，这是一种利用注意力机制来提高模型训练效率的模型。Transformer模型在某些特定任务上表现出色，如谷歌神经机器翻译模型。然而，它的最大优势在于其高度可扩展性，这得益于其并行化的特性。此外，Google Cloud也推荐使用Transformer模型作为参考模型，以便更好地利用Cloud TPU服务。接下来，我们将深入分析Transformer模型的工作原理，包括其基本组成和工作流程。\n"
     ]
    }
   ],
   "source": [
    "# 生成输出\n",
    "outputs = pipe(messages)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09d2bd",
   "metadata": {},
   "source": [
    "## 情景学习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25dfd3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用一个虚构单词的句子示例\n",
    "one_shot_prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"'嘻嘻哈哈'是一种传统的乐器，一个使用嘻嘻哈哈的句子是：\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"我有一把嘻嘻哈哈，这是我叔叔送我的生日礼物，我喜欢在家里弹奏它。\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"对某物进行'嘿哈'是指挥剑攻击它，一个使用嘿哈的句子是：\"\n",
    "    }\n",
    "]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37b1badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "'嘻嘻哈哈'是一种传统的乐器，一个使用嘻嘻哈哈的句子是：<|im_end|>\n",
      "<|im_start|>assistant\n",
      "我有一把嘻嘻哈哈，这是我叔叔送我的生日礼物，我喜欢在家里弹奏它。<|im_end|>\n",
      "<|im_start|>user\n",
      "对某物进行'嘿哈'是指挥剑攻击它，一个使用嘿哈的句子是：<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70d44c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嘿哈！这把剑真厉害，每次挥动都能打出好几下。\n"
     ]
    }
   ],
   "source": [
    "# 获取输出\n",
    "outputs = pipe(one_shot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf29882",
   "metadata": {},
   "source": [
    "## 链式提示：将问题分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abeed4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为了创建一个吸引人的名称和口号，我将考虑以下几个因素：\n",
      "\n",
      "1. **简洁性**：一个好的名字应该简单易记，能够传达核心信息。\n",
      "2. **独特性**：选择一个与众不同的名称可以增加品牌识别度。\n",
      "3. **创新性**：考虑到人工智能技术的发展趋势，创造一些新颖且具有吸引力的名字。\n",
      "4. **情感共鸣**：口号应能引起用户的共鸣，激发他们对特定主题的兴趣。\n",
      "\n",
      "基于以上考虑，这里有一个名为“智言通语”的名称，以及一个口号：“智言通语，智能对话”。这个名称既体现了AI技术的特点，又突出了其与用户沟通的方式。口号则通过强调“智言”（智慧的话语）和“通语”（交流的语言），表达了机器与人类之间的桥梁作用，同时也传递了智能化、便捷化的理念。\n",
      "\n",
      "这样的命名和口号不仅符合当前AI技术发展的潮流，还能够有效地吸引用户使用你的产品或服务。\n"
     ]
    }
   ],
   "source": [
    "# 为产品创建名称和口号\n",
    "product_prompt = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"为利用大型语言模型的聊天机器人创建一个名称和口号\"\n",
    "    }\n",
    "]\n",
    "outputs = pipe(product_prompt)\n",
    "product_description = outputs[0][\"generated_text\"]\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=500) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"智言通语，智能对话，让沟通更智能！\"\n"
     ]
    }
   ],
   "source": [
    "# 根据产品的名称和口号，生成一则销售口号\n",
    "sales_prompt = [\n",
    "    {\n",
    "        \"role\":\"user\", \n",
    "        \"content\":f\"为以下产品生成一个非常简短的推销口号：{product_description}\"\n",
    "    }  \n",
    "]\n",
    "outputs = pipe(sales_prompt)\n",
    "sales_pitch = outputs[0][\"generated_text\"]\n",
    "print(sales_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907fef9",
   "metadata": {},
   "source": [
    "# 使用生成模型进行推理\n",
    "## 思维链：回答前先思考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不作明确推理回答\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ed9aa",
   "metadata": {},
   "source": [
    "## 当前章节的内容省略\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a121f0",
   "metadata": {},
   "source": [
    "# 提示词设计原则\n",
    "1. Prompt generator：提示词不是一次写好的。可以先让大模型生成一版提示词，然后再去逐步迭代优化\n",
    "2. Be clear and dircet：提示词越清晰越好\n",
    "3. Use examples(multishot)：给模型更多的参考例子,例子和要求要统一\n",
    "4. Let Model think (chain for thought)：给模型思考的时间\n",
    "   1. think it step by step\n",
    "   2. <think>xxx 1.step 2.step</think> 明确每一步思考的内容\n",
    "5. Use XML tags：使用xml标签，尤其是大段的、 容易让大模型混乱的带有缩进之类的东西。标签可以提供清晰的边界\n",
    "6. Give Model a role (system prompt)：给模型一个具体的角色。电力工程师、数学老师...\n",
    "7. Prefill Model`s response：让模型在给定的内容之后接着生成（使用generate的API而不是response）\n",
    "8. Chain complex prompt：workflow，提示词和任务进行拆分，一类任务对应一个提示词\n",
    "9. Long context tips：长上下文的内容要放在instruction或prompt前面的部分。模型会优先遵循最近最近的一个指令"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
