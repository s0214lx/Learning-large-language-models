{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4a4f57",
   "metadata": {},
   "source": [
    "# 深入理解Transformer LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e28d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/openbayes/home/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec12160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eded44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712d3284d7e24025a49f8563082cf33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e88b2b3da54b58b59e8191629f0c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9010e3a9bfc479a9d9e9da9fd4c62c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86daf02724642ba9250e1c8d47eb21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff74c1ed30a4498ab0ba4dd4b1d76fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9298f3f4a53a4c088e59310ce5cf80fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa56bb72add2448daa024a6bae02bfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 从hugging face模型库下载并加载与模型匹配的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = 'auto', # 将模型自动匹配到可用的设备上\n",
    "    torch_dtype = \"auto\", # 自动选择最优的数据类型以节省显存并加速推理\n",
    "    trust_remote_code = True # 允许加载远程仓库中自定义的模型代码\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a437047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",# 指定任务类型为文本生成\n",
    "    model = model, # 使用加载的预训练模型对象\n",
    "    tokenizer = tokenizer, # 使用与模型匹配的分词器\n",
    "    return_full_text = False, # 控制返回的结果只包含新生成的内容\n",
    "    max_new_tokens = 50, # 最大新token数\n",
    "    do_sample = False, # 设置是否随机采样\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36bb23b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '？（ ） A. 李白 B. 王维 C. 杜甫 D. 白居易\\n\\n李白\\n\\n“我歌月徘徊，我舞影零乱”出自哪首诗？（ ） A'}]\n",
      "++++++++++++++++\n",
      "？（ ） A. 李白 B. 王维 C. 杜甫 D. 白居易\n",
      "\n",
      "李白\n",
      "\n",
      "“我歌月徘徊，我舞影零乱”出自哪首诗？（ ） A\n"
     ]
    }
   ],
   "source": [
    "prompt = \"《梦游天姥吟留别》的作者是谁\"\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output)\n",
    "print(\"++++++++++++++++\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db433c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个很有用的助手<|im_end|>\n",
      "<|im_start|>user\n",
      "《梦游天姥吟留别》的作者是谁<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    {\"role\":\"system\",\"content\": \"你是一个很有用的助手\"},\n",
    "    {\"role\":\"user\",\"content\":prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    message,\n",
    "    tokenize = False, # 表示只返回字符串形式，不进行token编码\n",
    "    add_generation_prompt = True # 如果不加 add_generation_prompt=True，模型可能不会开始生成 assistant 回复。\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9e2055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'李白'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(text)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405b4a0",
   "metadata": {},
   "source": [
    "## 输入格式：纯文本 vs 聊天模板（Prompt 形式）\n",
    "| 比较点                        | 纯文本 prompt    | Chat Template prompt      |\n",
    "| -------------------------- | ------------- | ------------------------- |\n",
    "| 输入格式                       | 用户直接写一句话      | system/user/assistant 结构化 |\n",
    "| 模型模式                       | 更像语言补全        | 对话问答模式                    |\n",
    "| 是否遵循 system 指令             | 视模型情况而定（通常较弱） | 强烈遵守                      |\n",
    "| 输出风格                       | 可能短、断句、补全式    | 更像聊天助手                    |\n",
    "| 是否需要 add_generation_prompt | 否             | 是（对话模型）                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3daa066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 896])\n",
      "torch.Size([1, 6, 151936])\n"
     ]
    }
   ],
   "source": [
    "# model output 可以怎么用\n",
    "prompt = \"The captial of France is\"\n",
    "\n",
    "input_ids = tokenizer(prompt,return_tensors='pt').input_ids.to(model.device)\n",
    "\n",
    "# model.model返回什么\n",
    "# model.model是其transformers主体，返回一个BaseModelOutputWithPast或类似对象，里面包含last_hidden_state，表示模型对每个token的向量表示\n",
    "model_output = model.model(input_ids)\n",
    "print(model_output[0].shape)\n",
    "\n",
    "# model.lm_head返回什么\n",
    "#lm_head是一个线性投影层，把隐藏层输出映射到词表大小的维度，用于生成每个token的预算分布\n",
    "lm_head_output = model.lm_head(model_output[0])\n",
    "print(lm_head_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192d6450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([151936])\n",
      " Paris\n"
     ]
    }
   ],
   "source": [
    "# model.lm_head的使用\n",
    "# 获取最后一个token的预测分布\n",
    "last_token_logits = lm_head_output[0,-1]\n",
    "print(last_token_logits.shape)\n",
    "# 获取概率最大的词\n",
    "predicted_token_id = last_token_logits.argmax().item()\n",
    "# 解码为词\n",
    "predicted_word = tokenizer.decode(predicted_token_id)\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53fc1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The captial of France is Paris, which has a population of 2.3 million people and an area of 15\n"
     ]
    }
   ],
   "source": [
    "# model.generate的使用\n",
    "output = model.generate(input_ids)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52527dc",
   "metadata": {},
   "source": [
    "| 方法                    | 用途                               | 输出             | 是否推荐用于生成    |\n",
    "| --------------------- | -------------------------------- | -------------- | ----------- |\n",
    "| `model.generate(...)` | 自动文本生成                           | token ids      | ✅ 推荐        |\n",
    "| `model.model(...)`    | 得到隐藏状态表示（不含输出头）                  | hidden states  | ❌ 不推荐直接用于生成 |\n",
    "| `model.lm_head(...)`  | 把 hidden states 映射到 vocab logits | logits（用于生成预测） | ✅ 可用于研究内部细节 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea98b113",
   "metadata": {},
   "source": [
    "## 推理方式：pipeline vs model.generate（调用方式）\n",
    "| 比较点                   | pipeline | model.generate |\n",
    "| --------------------- | -------- | -------------- |\n",
    "| 分词                    | 自动       | 手动             |\n",
    "| 解码                    | 自动       | 手动             |\n",
    "| 输出格式                  | 封装成 dict | 原始 tensor      |\n",
    "| 易用性                   | 简单       | 需要更多代码         |\n",
    "| 控制力                   | 中等       | 强              |\n",
    "| 是否可与 chat template 配合 | ✔ 可以     | ✔ 可以           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9446744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  KV cache 的原理，以及在推理的时候怎么使用？\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39746e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 7.72 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1min 18s ± 55.3 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "# %%timeit 是 IPython / Jupyter Notebook 的魔法命令，用于 测量代码块的运行时间。\n",
    "# -n 1 表示 只运行 1 次。默认情况下，timeit 会多次运行取平均，这里你只测一次。\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids = input_ids,\n",
    "    max_new_tokens = 1000,\n",
    "    use_cache = True # 启用缓存机制，在生成多个token时，避免重复计算历史token，加快速度\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=1000,\n",
    "  use_cache=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
